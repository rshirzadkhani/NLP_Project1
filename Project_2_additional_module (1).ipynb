{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 2-additional module.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8hq4A8VIxq0",
        "outputId": "7a313295-350d-4c7b-c617-46e10477953a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Removing @Mentions, URL links, Punctuations and numbers\n",
        "id_remove = lambda x: re.sub(r'@\\w+', '', x.lower())\n",
        "url_remove = lambda x: re.sub('https?://[A-Za-z0-9./]+','',x)\n",
        "alphanumeric = lambda x: re.sub(r\"\"\"\\w*\\d\\w*\"\"\", ' ', x)\n",
        "punc = lambda x: x.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Removing stop words and stem the words\n",
        "def normalize_corpus(corpus,text_stemming=False, stopword_removal=True, text_lemmatize=True, dict_check=True):\n",
        "    from nltk.tokenize.toktok import ToktokTokenizer\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokenizer = ToktokTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = set(nltk.corpus.words.words())\n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # remove extra newlines\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "        \n",
        "        # stem words\n",
        "        if text_stemming:\n",
        "            ps = nltk.stem.snowball.SnowballStemmer('english')\n",
        "            doc = ' '.join([ps.stem(word) for word in doc.split()])\n",
        "         \n",
        "        # lemmatize words\n",
        "        if text_lemmatize:\n",
        "            doc=' '.join([lemmatizer.lemmatize(word) for word in doc.split()])\n",
        "\n",
        "        # check if the words exist in english dictionary\n",
        "        if dict_check:\n",
        "            doc=\" \".join(w for w in nltk.wordpunct_tokenize(doc) \\\n",
        "                if w.lower() in words or not w.isalpha())\n",
        "            \n",
        "        # remove stop words\n",
        "        if stopword_removal:\n",
        "            tokens = tokenizer.tokenize(doc)\n",
        "            tokens = [token.strip() for token in tokens]\n",
        "            filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "            doc = ' '.join(filtered_tokens) \n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "   \n",
        "    return normalized_corpus\n",
        "\n",
        "# This function deletes empty documents after cleaning\n",
        "def delete_zero(df):\n",
        "    zero=[]\n",
        "    for i in df.index:\n",
        "        if len(df.text[i])==0:\n",
        "            zero.append(i)\n",
        "    df=df.drop(zero)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Preparing train & test data for 20 news group \n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove = ('headers', 'footers', 'quotes'))\n",
        "newsgroups_train = pd.DataFrame({'text': newsgroups_train.data, 'target': newsgroups_train.target})\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove = ('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = pd.DataFrame({'text': newsgroups_test.data, 'target': newsgroups_test.target})\n",
        "\n",
        "# Removing numbers, upper case letters and punctuations\n",
        "newsgroups_train['text'] = newsgroups_train.text.map(id_remove).map(url_remove).map(alphanumeric).map(punc)\n",
        "newsgroups_test['text'] = newsgroups_test.text.map(id_remove).map(url_remove).map(alphanumeric).map(punc)\n",
        "\n",
        "# Cleaning texts\n",
        "newsgroups_train['text']=normalize_corpus(newsgroups_train.text)\n",
        "newsgroups_test['text']=normalize_corpus(newsgroups_test.text)\n",
        "\n",
        "newsgroups_train=delete_zero(newsgroups_train)\n",
        "newsgroups_test=delete_zero(newsgroups_test)"
      ],
      "metadata": {
        "id": "-SJfKp2dI1-9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing train & test data for Sentiment140\n",
        "sentiment_train = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n",
        "                names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                encoding='latin-1')\n",
        "sentiment_train = sentiment_train.drop(columns=['id', 'date', 'query', 'user'])\n",
        "sentiment_train.polarity = sentiment_train.polarity.replace({0: 0, 4: 1})\n",
        "\n",
        "sentiment_test = pd.read_csv(\"new_testdata.manual.2009.06.14.csv\",\n",
        "                names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                encoding='latin-1')\n",
        "sentiment_test = sentiment_test.drop(columns=['id', 'date', 'query', 'user'])\n",
        "sentiment_test.polarity = sentiment_test.polarity.replace({0: 0, 4: 1})\n",
        "\n",
        "# sampling\n",
        "sentiment_train = sentiment_train.sample(n=50000)\n",
        "\n",
        "# Cleaning Data\n",
        "sentiment_train['text'] = sentiment_train['text'].astype('str')\n",
        "sentiment_test['text'] = sentiment_test['text'].astype('str')\n",
        "sentiment_train['text'] = sentiment_train.text.map(id_remove).map(url_remove).map(alphanumeric).map(punc)\n",
        "sentiment_test['text'] = sentiment_test.text.map(id_remove).map(url_remove).map(alphanumeric).map(punc)\n",
        "sentiment_train['text']=normalize_corpus(sentiment_train.text)\n",
        "sentiment_test['text']=normalize_corpus(sentiment_test.text)\n",
        "sentiment_train=delete_zero(sentiment_train)\n",
        "sentiment_test=delete_zero(sentiment_test)"
      ],
      "metadata": {
        "id": "W7EkuXjqI6i0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        N, D = x.shape\n",
        "        C = np.max(y) + 1\n",
        "        mu,mu2, sigma = np.zeros((C,D)), np.zeros((C,D)), np.zeros((C,D))\n",
        "        Nc = np.zeros(C) \n",
        "        for c in range(C):\n",
        "            x_c = x[y == c]                           \n",
        "            Nc[c] = x_c.shape[0]                      \n",
        "            x_c_squared = x_c.copy()\n",
        "            x_c_squared.data **= 2\n",
        "            for ic in range(D):               \n",
        "                mu[c,ic] = (x_c[:,ic].sum()+1) / (Nc[c]+C)\n",
        "                mu2[c,ic]=((x_c_squared[:,ic]).sum()+1) / (Nc[c]+C)\n",
        "                sigma[c,ic]=np.sqrt(mu2[c,ic]-mu[c,ic]**2)\n",
        "        self.mu = mu                                  \n",
        "        self.sigma = sigma                         \n",
        "        self.pi =(Nc+1)/(N+C)\n",
        "        return self\n",
        "\n",
        "\n",
        "def logsumexp(Z):                                                \n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              \n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    Nt, D = xt.shape\n",
        "    log_prior = np.log(self.pi)[:, None] \n",
        "    C,Dt=self.mu.shape\n",
        "    log_likelihood=np.zeros((C,Nt))\n",
        "    for i in range(Nt):\n",
        "          xt_i=xt.tocsr()[i,:].todense()\n",
        "          for j in range(C):\n",
        "              log_liklihood_i=-.5 * np.log(2*np.pi) - np.log(self.sigma[j,:]) -.5 * (np.power((xt_i - self.mu[j,:])/self.sigma[j,:],2))\n",
        "              log_likelihood[j,i] = np.sum(log_liklihood_i)\n",
        "    log_posterior = log_prior + log_likelihood\n",
        "    posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "    y_pred = np.argmax(posterior.T, 1)\n",
        "    return posterior.T,y_pred\n",
        "    \n",
        "GaussianNaiveBayes.predict = predict"
      ],
      "metadata": {
        "id": "xHzu85KgJPR_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha=alpha\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        N, D = x.shape\n",
        "        C = np.max(y) + 1\n",
        "        self.theta = np.zeros((C,D))\n",
        "        self.prior = np.zeros(C)\n",
        "        for c in range(C):\n",
        "            x_c = x[y == c]\n",
        "            self.theta[c,:] = (np.sum(x_c, 0) + self.alpha) / (np.sum(x_c) + self.alpha * D)\n",
        "            self.prior[c] = x_c.shape[0] / len(y)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x_test): \n",
        "        N = x_test.shape[0]\n",
        "        log_prior = np.log(self.prior)[:, None]\n",
        "        C,Dt=self.theta.shape\n",
        "        log_likelihood=np.zeros((C,N))\n",
        "        log_theta=np.log(self.theta)\n",
        "        for i in range(N):\n",
        "             xti=x_test.tocsr()[i,:].todense()\n",
        "             for j in range(C):\n",
        "                 log_likelihood[j,i] = np.dot(xti,log_theta[j,:])\n",
        "        log_posterior = log_prior + log_likelihood\n",
        "        posterior = log_posterior\n",
        "        y_pred = np.argmax(posterior.T, 1)\n",
        "        return posterior.T, y_pred "
      ],
      "metadata": {
        "id": "vWHuHSuNJP5K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc( x_test, y_test, y_pred):\n",
        "  accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
        "  TP1=np.where(y_test==1 )\n",
        "  TP2=np.where(y_pred == y_test)\n",
        "  TP=np.intersect1d(TP1,TP2)\n",
        "  FN1=np.where(y_test==1 )\n",
        "  FN2=np.where(y_pred==0 )\n",
        "  FN=np.intersect1d(FN1,FN2)\n",
        "  FP1=np.where(y_test==0 )\n",
        "  FP2=np.where(y_pred==1 )\n",
        "  FP=np.intersect1d(FP1,FP2)\n",
        "  TN1=np.where(y_test==0 )\n",
        "  TN2=np.where(y_pred==0 )\n",
        "  TN=np.intersect1d(TN1,TN2)\n",
        "  recall = TP.shape[0]/(FN.shape[0]+TP.shape[0]+0.001)\n",
        "  precision=TP.shape[0]/(FP.shape[0]+TP.shape[0]+0.001)\n",
        "  Selectivity=TN.shape[0]/(TN.shape[0]+FP.shape[0]+0.001)\n",
        "  N_predictive_v=TN.shape[0]/(TN.shape[0]+FN.shape[0]+0.001)\n",
        "  return accuracy,recall,precision,Selectivity,N_predictive_v"
      ],
      "metadata": {
        "id": "p_aiKCQVKYsT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_train1=sentiment_train.sample(n=1000)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=5)\n",
        "vectors_train = vectorizer.fit_transform(sentiment_train1.text)\n",
        "vectors_test = vectorizer.transform(sentiment_test.text)\n",
        "\n",
        "x_train, y_train = vectors_train, sentiment_train1.polarity.to_numpy()\n",
        "x_test, y_test= vectors_test, sentiment_test.polarity.to_numpy()\n",
        "model = GaussianNaiveBayes()\n",
        "model.fit(x_train, y_train)\n",
        "y_prob, y_pred = model.predict(x_test)\n",
        "accuracy,_,_,_,_=evaluate_acc(x_test,y_test,y_pred)\n",
        "print(f'test accuracy: {accuracy}')\n",
        "\n",
        "confusion_word_inex=[]\n",
        "Nt,D=np.shape(vectors_train)\n",
        "for i in range(D):\n",
        "    x_train, y_train = vectors_train.tocsr()[:,i], sentiment_train1.polarity.to_numpy()\n",
        "    x_test, y_test= vectors_test.tocsr()[:,i], sentiment_test.polarity.to_numpy()\n",
        "    model = GaussianNaiveBayes()\n",
        "    model.fit(x_train, y_train)\n",
        "    y_prob, y_pred = model.predict(x_test)\n",
        "    accuracy,_,_,_,_ = evaluate_acc(x_test,y_test,y_pred)\n",
        "    if accuracy<0.5:\n",
        "        confusion_word_inex.append(i)\n",
        "        #print('yes')\n",
        "feature_word=vectorizer.get_feature_names()    \n",
        "Confusion_word=[]\n",
        "Dc=len(confusion_word_inex)\n",
        "for i in range(Dc):\n",
        "    kkk=confusion_word_inex[i]\n",
        "    Confusion_word.append(feature_word[confusion_word_inex[i]])\n",
        "def clean_confusion(corpus,Confusion_word):\n",
        "    clean_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        doc1=''\n",
        "        for word in doc.split():\n",
        "            if word not in Confusion_word:\n",
        "                doc1=doc1+' '+word\n",
        "        clean_corpus.append(doc1)\n",
        "    return clean_corpus\n",
        "            \n",
        "print(len(Confusion_word))\n",
        "\n",
        "sentiment_train['text']=clean_confusion(sentiment_train.text,Confusion_word)\n",
        "sentiment_test['text']=clean_confusion(sentiment_test.text,Confusion_word)\n",
        "\n",
        "def delete_zero(df):\n",
        "    zero=[]\n",
        "    for i in df.index:\n",
        "        if len(df.text[i])<2:\n",
        "            zero.append(i)\n",
        "    df=df.drop(zero)\n",
        "    return df\n",
        "\n",
        "sentiment_test=delete_zero(sentiment_test)\n",
        "sentiment_train=delete_zero(sentiment_train)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=5)\n",
        "vectors_train = vectorizer.fit_transform(sentiment_train.text)\n",
        "vectors_test = vectorizer.transform(sentiment_test.text)\n",
        "\n",
        "\n",
        "    \n",
        "x_train, y_train = vectors_train, sentiment_train.polarity.to_numpy()\n",
        "x_test, y_test= vectors_test, sentiment_test.polarity.to_numpy()\n",
        "model = GaussianNaiveBayes()\n",
        "model.fit(x_train, y_train)\n",
        "y_prob, y_pred = model.predict(x_test)\n",
        "\n",
        "accuracy,_,_,_,_ = evaluate_acc(x_test,y_test,y_pred)\n",
        "print(f'test accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLdbl5LOJTAj",
        "outputId": "23020df8-75d1-4e1b-e965-ce513ab239b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6836158192090396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122\n",
            "test accuracy: 0.7082152974504249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_train1=sentiment_train.sample(n=1000)\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=5)\n",
        "vectors_train = vectorizer.fit_transform(sentiment_train1.text)\n",
        "vectors_test = vectorizer.transform(sentiment_test.text)\n",
        "\n",
        "x_train, y_train = vectors_train, sentiment_train1.polarity.to_numpy()\n",
        "x_test, y_test= vectors_test, sentiment_test.polarity.to_numpy()\n",
        "model = MultinomialNaiveBayes()\n",
        "model.fit(x_train, y_train)\n",
        "y_prob, y_pred = model.predict(x_test)\n",
        "accuracy,_,_,_,_=evaluate_acc(x_test,y_test,y_pred)\n",
        "print(f'test accuracy: {accuracy}')\n",
        "\n",
        "confusion_word_inex=[]\n",
        "Nt,D=np.shape(vectors_train)\n",
        "for i in range(D):\n",
        "    x_train, y_train = vectors_train.tocsr()[:,i], sentiment_train1.polarity.to_numpy()\n",
        "    x_test, y_test= vectors_test.tocsr()[:,i], sentiment_test.polarity.to_numpy()\n",
        "    model = GaussianNaiveBayes()\n",
        "    model.fit(x_train, y_train)\n",
        "    y_prob, y_pred = model.predict(x_test)\n",
        "    accuracy,_,_,_,_ = evaluate_acc(x_test,y_test,y_pred)\n",
        "    if accuracy<0.5:\n",
        "        confusion_word_inex.append(i)\n",
        "        #print('yes')\n",
        "feature_word=vectorizer.get_feature_names()    \n",
        "Confusion_word=[]\n",
        "Dc=len(confusion_word_inex)\n",
        "for i in range(Dc):\n",
        "    kkk=confusion_word_inex[i]\n",
        "    Confusion_word.append(feature_word[confusion_word_inex[i]])\n",
        "def clean_confusion(corpus,Confusion_word):\n",
        "    clean_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        doc1=''\n",
        "        for word in doc.split():\n",
        "            if word not in Confusion_word:\n",
        "                doc1=doc1+' '+word\n",
        "        clean_corpus.append(doc1)\n",
        "    return clean_corpus\n",
        "            \n",
        "print(len(Confusion_word))\n",
        "\n",
        "sentiment_train['text']=clean_confusion(sentiment_train.text,Confusion_word)\n",
        "sentiment_test['text']=clean_confusion(sentiment_test.text,Confusion_word)\n",
        "\n",
        "def delete_zero(df):\n",
        "    zero=[]\n",
        "    for i in df.index:\n",
        "        if len(df.text[i])<2:\n",
        "            zero.append(i)\n",
        "    df=df.drop(zero)\n",
        "    return df\n",
        "\n",
        "sentiment_test=delete_zero(sentiment_test)\n",
        "sentiment_train=delete_zero(sentiment_train)\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=5)\n",
        "vectors_train = vectorizer.fit_transform(sentiment_train.text)\n",
        "vectors_test = vectorizer.transform(sentiment_test.text)\n",
        "\n",
        "\n",
        "    \n",
        "x_train, y_train = vectors_train, sentiment_train.polarity.to_numpy()\n",
        "x_test, y_test= vectors_test, sentiment_test.polarity.to_numpy()\n",
        "model = MultinomialNaiveBayes()\n",
        "model.fit(x_train, y_train)\n",
        "y_prob, y_pred = model.predict(x_test)\n",
        "\n",
        "accuracy,_,_,_,_ = evaluate_acc(x_test,y_test,y_pred)\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2lUJ272RAJb",
        "outputId": "c2567ede-39cf-4722-bc9f-6f37970a66c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6751412429378532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113\n",
            "test accuracy: 0.7840909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectorizer = CountVectorizer(max_df=900,min_df=50)\n",
        "newsgroups_train1=newsgroups_train.sample(n=1500)\n",
        "newsgroups_test1=newsgroups_test.sample(n=1000)\n",
        "\n",
        "vectors_train = vectorizer.fit_transform(newsgroups_train1.text)\n",
        "vectors_test = vectorizer.transform(newsgroups_test1.text)\n",
        "\n",
        "x_train, y_train = vectors_train, newsgroups_train1.target.to_numpy()\n",
        "x_test, y_test= vectors_test, newsgroups_test1.target.to_numpy()\n",
        "model_MNB = MultinomialNaiveBayes()\n",
        "model_MNB.fit(x_train, y_train)\n",
        "y_prob_MNB, y_pred_MNB = model_MNB.predict(x_test)\n",
        "accuracy_MNB,recall_MNB,precision_MNB,Selectivity_MNB,N_predictive_v_MNB=evaluate_acc(x_test, y_test, y_pred_MNB)\n",
        "print(f'test accuracy: {accuracy_MNB}') \n",
        "\n",
        "confusion_word_inex=[]\n",
        "Nt,D=np.shape(vectors_train)\n",
        "for i in range(D):\n",
        "    x_train, y_train = vectors_train.tocsr()[:,i], newsgroups_train1.target.to_numpy()\n",
        "    x_test, y_test= vectors_test.tocsr()[:,i], newsgroups_test1.target.to_numpy()\n",
        "    model = MultinomialNaiveBayes()\n",
        "    model.fit(x_train, y_train)\n",
        "    y_prob, y_pred = model.predict(x_test)\n",
        "    #y_pred = np.argmax(y_prob, 1)\n",
        "    accuracy = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
        "    if accuracy< 0.1:\n",
        "        confusion_word_inex.append(i)\n",
        "        #print('yes')\n",
        "        \n",
        "feature_word=vectorizer.get_feature_names()    \n",
        "Confusion_word=['dd']\n",
        "Dc=len(confusion_word_inex)\n",
        "for i in range(Dc):\n",
        "    kkk=confusion_word_inex[i]\n",
        "    Confusion_word.append(feature_word[confusion_word_inex[i]])\n",
        "def clean_confusion(corpus,Confusion_word):\n",
        "    clean_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        doc1=''\n",
        "        for word in doc.split():\n",
        "            if word not in Confusion_word:\n",
        "                doc1=doc1+' '+word\n",
        "        clean_corpus.append(doc1)\n",
        "    return clean_corpus\n",
        "            \n",
        "#%%\n",
        "newsgroups_train['text']=clean_confusion(newsgroups_train.text,Confusion_word)\n",
        "newsgroups_test['text']=clean_confusion(newsgroups_test.text,Confusion_word)\n",
        "\n",
        "\n",
        "def delete_zero(df):\n",
        "    zero=[]\n",
        "    for i in df.index:\n",
        "        if len(df.text[i])<2:\n",
        "            zero.append(i)\n",
        "    df=df.drop(zero)\n",
        "    return df\n",
        "\n",
        "newsgroups_test2=delete_zero(newsgroups_test)\n",
        "newsgroups_train=delete_zero(newsgroups_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer(max_df=900, min_df=5)\n",
        "count_vectors_train = count_vectorizer.fit_transform(newsgroups_train.text)\n",
        "count_vectors_test = count_vectorizer.transform(newsgroups_test.text)\n",
        "    \n",
        "x_train, y_train = count_vectors_train, newsgroups_train.target.to_numpy()\n",
        "x_test, y_test= count_vectors_test, newsgroups_test.target.to_numpy()\n",
        "model_MNB = MultinomialNaiveBayes()\n",
        "model_MNB.fit(x_train, y_train)\n",
        "y_prob_MNB, y_pred_MNB = model_MNB.predict(x_test)\n",
        "accuracy_MNB,recall_MNB,precision_MNB,Selectivity_MNB,N_predictive_v_MNB=evaluate_acc(x_test, y_test, y_pred_MNB)\n",
        "print(f'test accuracy: {accuracy_MNB}') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXnNnnk5R5wU",
        "outputId": "215a612a-9d1e-4f28-be44-abfb0bbcce3b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6150570133260064\n"
          ]
        }
      ]
    }
  ]
}